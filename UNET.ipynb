{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bfa0961-cdde-415d-8138-15c1f1489387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /srv/conda/envs/notebook/lib/python3.9/site-packages (1.3.0)\n",
      "Requirement already satisfied: PyYAML in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (5.4.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (4.7.0.68)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (0.19.3)\n",
      "Requirement already satisfied: scipy in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (1.23.3)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from qudida>=0.0.4->albumentations) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions in /srv/conda/envs/notebook/lib/python3.9/site-packages (from qudida>=0.0.4->albumentations) (4.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (2.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (9.2.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (2022.8.12)\n",
      "Requirement already satisfied: networkx>=2.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (2.8.6)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d516103f-ddc2-41be-8d56-e12d9d41275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from Dataloaders import supervised_loader, test_loader, supervised_train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b8a8a3-960a-4327-b697-6992b79df55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30aedfba-4dd9-454d-98d2-bbb1c7c6f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a236e03e-67bb-420c-8e65-dda3c6abecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 736 \n",
    "IMAGE_WIDTH = 1280 \n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"images\"\n",
    "TRAIN_MASK_DIR = \"thims\"\n",
    "VAL_IMG_DIR = \"val_im\"\n",
    "VAL_MASK_DIR = \"val_lab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64053b62-2282-494d-ba9d-102b8cc22cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8286e9a-7dba-48e2-9fa2-89d355e9bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data_size = 250\n",
    "# ranset = set()\n",
    "# while (len(ranset) <= val_data_size):\n",
    "#     ranset.add(np.random.randint(0, 2001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "843f2151-ad91-4a63-9abd-e3eefbaa7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# imlis = os.listdir(\"images\")\n",
    "# lablis = os.listdir(\"thims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d051b4d-a61c-4d70-b74b-9627918d3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ranset:\n",
    "#     shutil.move(f\"images/{imlis[i]}\", \"val_im\")\n",
    "#     shutil.move(f\"thims/{lablis[i]}\", \"val_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ac25e2e-f3f6-45d0-ad6d-230b5dd94fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading dataloaders\n",
    "\n",
    "train_data_loader = supervised_loader(image_dir = TRAIN_IMG_DIR, mask_dir= TRAIN_MASK_DIR,\n",
    "                                      transform=supervised_train_transform, batch_size=BATCH_SIZE, shuffle=True, num_workers=4,\n",
    "                                      shape = (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "\n",
    "val_data_loader = supervised_loader(image_dir = VAL_IMG_DIR, mask_dir= VAL_MASK_DIR,\n",
    "                                      transform=test_transform, batch_size=BATCH_SIZE, shuffle=True, num_workers = 4,\n",
    "                                      shape = (IMAGE_WIDTH, IMAGE_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1b7d2-2322-4bb7-be2b-d5f643feada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 71/219 [00:53<01:49,  1.36it/s, iou_loss=tensor(0.5995), loss=-.847] "
     ]
    }
   ],
   "source": [
    "iou_loss_fn = torchmetrics.classification.JaccardIndex(task = 'binary', threshold = 0., num_classes = 2)\n",
    "\n",
    "epochs = (range(NUM_EPOCHS))\n",
    "\n",
    "val_loss_lis = []\n",
    "train_loss_lis = []\n",
    "\n",
    "train_iou = []\n",
    "val_iou = []\n",
    "\n",
    "for epoch in epochs:\n",
    "    \n",
    "    total_validation_loss = 0\n",
    "    total_train_loss = 0\n",
    "    total_train_iou = 0\n",
    "    total_val_iou = 0\n",
    "    \n",
    "    loop = tqdm(train_data_loader)\n",
    "\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        iou_train_loss = iou_loss_fn(predictions.to(\"cpu\"), ((targets > 0)*1).to(\"cpu\"))\n",
    "        total_train_iou += iou_train_loss\n",
    "        loop.set_postfix(loss=loss.item(), iou_loss = iou_train_loss)\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    train_loss_lis.append(total_train_loss)\n",
    "    train_iou.append(total_train_iou)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loop = tqdm(val_data_loader)\n",
    "        for (data, targets) in val_loop:\n",
    "            data = data.to(device=DEVICE)\n",
    "            targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "            total_validation_loss += loss\n",
    "            iou_val_loss = iou_loss_fn(predictions.to(\"cpu\"), ((targets > 0)*1).to(\"cpu\"))\n",
    "            total_val_iou += iou_val_loss\n",
    "            val_loop.set_postfix(val_loss=loss.item(), iou_loss = iou_val_loss)\n",
    "    \n",
    "    val_loss_lis.append(total_validation_loss)\n",
    "    val_iou.append(total_val_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f20b6d-dd68-4c12-8da7-81028a54848f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f362f0-70f9-4db4-a29f-e81c723be43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76434b-1158-4771-92cc-9fcc9e49ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d7842-e82f-4b0d-aa5b-193e6533509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb6fa7-a77d-427c-87f1-95f967a62f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4182c7-e389-4ba4-87ce-3df27dbbfdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee72772-a12f-4433-9ed7-3df2c9e8538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"unet1750_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2234437-7eb1-4c24-a699-53ce9efa2bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
