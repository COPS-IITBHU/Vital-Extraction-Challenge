{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bfa0961-cdde-415d-8138-15c1f1489387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /srv/conda/envs/notebook/lib/python3.9/site-packages (1.3.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (4.7.0.68)\n",
      "Requirement already satisfied: PyYAML in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (5.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (0.19.3)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (1.23.3)\n",
      "Requirement already satisfied: scipy in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (1.9.1)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from qudida>=0.0.4->albumentations) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions in /srv/conda/envs/notebook/lib/python3.9/site-packages (from qudida>=0.0.4->albumentations) (4.3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (2022.8.12)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (2.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied: networkx>=2.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (2.8.6)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (9.2.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77548a0-57f2-422a-93ec-3ed1a45e2544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /srv/conda/envs/notebook/lib/python3.9/site-packages (0.9.3)\n",
      "Requirement already satisfied: torch>=1.3.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from torchmetrics) (1.12.1.post200)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from torchmetrics) (1.23.3)\n",
      "Requirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing_extensions in /srv/conda/envs/notebook/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d516103f-ddc2-41be-8d56-e12d9d41275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b8a8a3-960a-4327-b697-6992b79df55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30aedfba-4dd9-454d-98d2-bbb1c7c6f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a236e03e-67bb-420c-8e65-dda3c6abecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 736 \n",
    "IMAGE_WIDTH = 1280 \n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"images\"\n",
    "TRAIN_MASK_DIR = \"thims\"\n",
    "VAL_IMG_DIR = \"val_im\"\n",
    "VAL_MASK_DIR = \"val_lab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f08a1157-7348-4049-a9ea-d4df329bbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "        image = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        mask = mask[:, :, 0]\n",
    "         \n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "class TestSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        image = cv2.imread(img_path)\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image)\n",
    "            image = augmentations[\"image\"]\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40aef569-238a-40bb-b754-9f6de4be6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p = 1.0), \n",
    "    A.Rotate(limit=45, p = 0.7),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64053b62-2282-494d-ba9d-102b8cc22cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8286e9a-7dba-48e2-9fa2-89d355e9bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data_size = 250\n",
    "# ranset = set()\n",
    "# while (len(ranset) <= val_data_size):\n",
    "#     ranset.add(np.random.randint(0, 2001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "843f2151-ad91-4a63-9abd-e3eefbaa7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# imlis = os.listdir(\"images\")\n",
    "# lablis = os.listdir(\"thims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d051b4d-a61c-4d70-b74b-9627918d3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ranset:\n",
    "#     shutil.move(f\"images/{imlis[i]}\", \"val_im\")\n",
    "#     shutil.move(f\"thims/{lablis[i]}\", \"val_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ac25e2e-f3f6-45d0-ad6d-230b5dd94fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SegmentationDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, transform)\n",
    "val_data = SegmentationDataset(VAL_IMG_DIR, VAL_MASK_DIR, transform)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True,\n",
    "                                             num_workers = 4)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True,\n",
    "                                             num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1b7d2-2322-4bb7-be2b-d5f643feada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [02:44<00:00,  1.33it/s, iou_loss=tensor(0.8879), loss=-221]\n",
      "100%|██████████| 32/32 [00:17<00:00,  1.84it/s, iou_loss=tensor(0.9243), val_loss=-232]\n",
      " 34%|███▍      | 74/219 [00:56<01:48,  1.34it/s, iou_loss=tensor(0.9142), loss=-255]"
     ]
    }
   ],
   "source": [
    "iou_loss_fn = torchmetrics.classification.JaccardIndex(task = 'binary', threshold = 0., num_classes = 2)\n",
    "\n",
    "epochs = (range(NUM_EPOCHS))\n",
    "\n",
    "val_loss_lis = []\n",
    "train_loss_lis = []\n",
    "\n",
    "train_iou = []\n",
    "val_iou = []\n",
    "\n",
    "for epoch in epochs:\n",
    "    \n",
    "    total_validation_loss = 0\n",
    "    total_train_loss = 0\n",
    "    total_train_iou = 0\n",
    "    total_val_iou = 0\n",
    "    \n",
    "    loop = tqdm(train_data_loader)\n",
    "\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        iou_train_loss = iou_loss_fn(predictions.to(\"cpu\"), ((targets > 0)*1).to(\"cpu\"))\n",
    "        total_train_iou += iou_train_loss\n",
    "        loop.set_postfix(loss=loss.item(), iou_loss = iou_train_loss)\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    train_loss_lis.append(total_train_loss)\n",
    "    train_iou.append(total_train_iou)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loop = tqdm(val_data_loader)\n",
    "        for (data, targets) in val_loop:\n",
    "            data = data.to(device=DEVICE)\n",
    "            targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "            total_validation_loss += loss\n",
    "            iou_val_loss = iou_loss_fn(predictions.to(\"cpu\"), ((targets > 0)*1).to(\"cpu\"))\n",
    "            total_val_iou += iou_val_loss\n",
    "            val_loop.set_postfix(val_loss=loss.item(), iou_loss = iou_val_loss)\n",
    "    \n",
    "    val_loss_lis.append(total_validation_loss)\n",
    "    val_iou.append(total_val_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f362f0-70f9-4db4-a29f-e81c723be43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_loss_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76434b-1158-4771-92cc-9fcc9e49ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_loss_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d7842-e82f-4b0d-aa5b-193e6533509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(val_loss_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb6fa7-a77d-427c-87f1-95f967a62f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(val_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4182c7-e389-4ba4-87ce-3df27dbbfdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee72772-a12f-4433-9ed7-3df2c9e8538d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
