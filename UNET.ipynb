{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa0961-cdde-415d-8138-15c1f1489387",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d516103f-ddc2-41be-8d56-e12d9d41275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from Dataloaders import supervised_loader, test_loader, supervised_train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b8a8a3-960a-4327-b697-6992b79df55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30aedfba-4dd9-454d-98d2-bbb1c7c6f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a236e03e-67bb-420c-8e65-dda3c6abecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 736 \n",
    "IMAGE_WIDTH = 1280 \n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"segmentation_data/train_imgs\"\n",
    "TRAIN_MASK_DIR = \"segmentation_data/train_masks\"\n",
    "VAL_IMG_DIR = \"segmentation_data/val_imgs\"\n",
    "VAL_MASK_DIR = \"segmentation_data/val_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64053b62-2282-494d-ba9d-102b8cc22cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8286e9a-7dba-48e2-9fa2-89d355e9bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data_size = 250\n",
    "# ranset = set()\n",
    "# while (len(ranset) <= val_data_size):\n",
    "#     ranset.add(np.random.randint(0, 2001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "843f2151-ad91-4a63-9abd-e3eefbaa7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# imlis = os.listdir(\"images\")\n",
    "# lablis = os.listdir(\"thims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d051b4d-a61c-4d70-b74b-9627918d3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ranset:\n",
    "#     shutil.move(f\"images/{imlis[i]}\", \"val_im\")\n",
    "#     shutil.move(f\"thims/{lablis[i]}\", \"val_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac25e2e-f3f6-45d0-ad6d-230b5dd94fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading dataloaders\n",
    "\n",
    "train_data_loader = supervised_loader(image_dir = TRAIN_IMG_DIR, mask_dir= TRAIN_MASK_DIR,\n",
    "                                      transform=supervised_train_transform, batch_size=BATCH_SIZE, shuffle=True, num_workers=4,\n",
    "                                      shape = (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "\n",
    "val_data_loader = supervised_loader(image_dir = VAL_IMG_DIR, mask_dir= VAL_MASK_DIR,\n",
    "                                      transform=test_transform, batch_size=BATCH_SIZE, shuffle=False, num_workers = 4,\n",
    "                                      shape = (IMAGE_WIDTH, IMAGE_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f1b7d2-2322-4bb7-be2b-d5f643feada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch - 0, Step - 57/225, Training_Loss - 0.5003, Train_mIOU - 0.5895\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@45.248] global loadsave.cpp:244 findDecoder imread_('segmentation_data/train_imgs/medicakolkata_ccu2_mon--8_2022_6_20_3_4_44.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch - 0, Step - 58/225, Training_Loss - 0.4975, Train_mIOU - 0.5939\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@45.779] global loadsave.cpp:244 findDecoder imread_('segmentation_data/train_imgs/kamalnayanbajaj_micu_mon--22_2022_6_20_21_47_17.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch - 0, Step - 59/225, Training_Loss - 0.4951, Train_mIOU - 0.5974\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@46.477] global loadsave.cpp:244 findDecoder imread_('segmentation_data/train_imgs/medicakolkata_ccu3_mon--2_2022_6_8_16_2_30.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch - 0, Step - 60/225, Training_Loss - 0.4924, Train_mIOU - 0.6017\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@47.170] global loadsave.cpp:244 findDecoder imread_('segmentation_data/train_imgs/medicakolkata_ccu2_mon--8_2022_6_20_18_5_13.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch - 0, Step - 61/225, Training_Loss - 0.49, Train_mIOU - 0.6053\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@47.867] global loadsave.cpp:244 findDecoder imread_('segmentation_data/train_imgs/medicakolkata_ccu2_mon--4_2022_6_20_6_3_58.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch - 0, Step - 62/225, Training_Loss - 0.4874, Train_mIOU - 0.6094\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@48.553] global loadsave.cpp:244 findDecoder imread_('segmentation_data/train_imgs/medicakolkata_ccu2_mon--8_2022_5_18_6_6_53.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch - 0, Step - 63/225, Training_Loss - 0.4849, Train_mIOU - 0.6132\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@49.267] global loadsave.cpp:244 findDecoder imread_('segmentation_data/train_imgs/medicakolkata_ccu3_mon--2_2022_5_18_22_2_47.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch - 0, Step - 64/225, Training_Loss - 0.4824, Train_mIOU - 0.6171\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@49.963] global loadsave.cpp:244 findDecoder imread_('segmentation_data/train_imgs/hcgnagpur_icu_mon--1_2022_6_20_18_30_1.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch - 0, Step - 65/225, Training_Loss - 0.4799, Train_mIOU - 0.6209\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@50.660] global loadsave.cpp:244 findDecoder imread_('segmentation_data/train_imgs/medicakolkata_ccu3_mon--5_2022_5_18_3_3_0.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Caught error in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jovyan/PlanetaryComputerExamples/cloudphy/Dataloaders.py\", line 23, in __getitem__\n    image = cv2.resize(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB), self.shape)\ncv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m total_val_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m## training part\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_data_loader):\n\u001b[1;32m     22\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[1;32m     23\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1376\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1402\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jovyan/PlanetaryComputerExamples/cloudphy/Dataloaders.py\", line 23, in __getitem__\n    image = cv2.resize(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB), self.shape)\ncv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n"
     ]
    }
   ],
   "source": [
    "iou_fn = torchmetrics.classification.JaccardIndex(task = 'binary', threshold = 0., num_classes = 2)\n",
    "\n",
    "epochs = (range(NUM_EPOCHS))\n",
    "\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "\n",
    "train_iou = []\n",
    "val_iou = []\n",
    "\n",
    "max_val_iou = 0\n",
    "for epoch in epochs:\n",
    "    \n",
    "    total_val_loss = 0\n",
    "    total_train_loss = 0\n",
    "    total_train_iou = 0\n",
    "    total_val_iou = 0\n",
    "    \n",
    "    ## training part\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_data_loader):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        iou_train = iou_fn(predictions.to(\"cpu\"), ((targets > 0)*1).to(\"cpu\"))\n",
    "        total_train_iou += iou_train.item()\n",
    "        # loop.set_postfix(loss=loss.item(), iou_loss = iou_train_loss)\n",
    "        total_train_loss += loss.item()\n",
    "        print(f\" Epoch - {epoch}, Step - {batch_idx+1}/{len(train_data_loader)}, Training_Loss - {round(total_train_loss/(batch_idx+1), 4)}, Train_mIOU - {round(total_train_iou/(batch_idx+1), 4)}\", end=\"\\r\")\n",
    "    train_loss.append(total_train_loss/(batch_idx+1))\n",
    "    train_iou.append(total_train_iou/(batch_idx+1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(val_data_loader):\n",
    "            data = data.to(device=DEVICE)\n",
    "            targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "            total_validation_loss += loss\n",
    "            iou_val = iou_fn(predictions.to(\"cpu\"), ((targets > 0)*1).to(\"cpu\"))\n",
    "            total_val_iou += iou_val.item()\n",
    "            # val_loop.set_postfix(val_loss=loss.item(), iou_loss = iou_val_loss)\n",
    "    print(f\" Epoch - {epoch}, Training_Loss - {round(total_train_loss/(batch_idx+1), 4)}, Train_mIOU - {round(total_train_iou/(batch_idx+1), 4)}, Training_Loss - {round(total_val_loss/(batch_idx+1), 4)}, val_mIOU - {round(total_val_iou/(batch_idx+1), 4)}\")\n",
    "    val_loss.append(total_val_loss/(batch_idx+1))\n",
    "    val_iou.append(total_val_loss/(batch_idx+1))\n",
    "    \n",
    "    if total_val_loss/(batch_idx+1)>max_val_iou:\n",
    "        print(f\"Validation mIOU increased from {max_val_iou} to {total_val_loss/(batch_idx+1)}, Saving Model....\")\n",
    "        torch.save(model.state_dict(), \"best_model.ckpt\")\n",
    "        max_val_iou = total_val_loss/(batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f20b6d-dd68-4c12-8da7-81028a54848f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f362f0-70f9-4db4-a29f-e81c723be43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76434b-1158-4771-92cc-9fcc9e49ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d7842-e82f-4b0d-aa5b-193e6533509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb6fa7-a77d-427c-87f1-95f967a62f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4182c7-e389-4ba4-87ce-3df27dbbfdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee72772-a12f-4433-9ed7-3df2c9e8538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"unet1750_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2234437-7eb1-4c24-a699-53ce9efa2bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
