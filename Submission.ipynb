{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install albumentations\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STANDARD LIBRARY AND PIPELINE MODULES IMPORT\n",
    "\n",
    "\n",
    "##IMPORTING IMAGE LOADING AND PLOTTING LIBS\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "## IMPORTING TORCH FOR ML-PART\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## IMPORT ALBUMENTATION AS DATA AUGMENTATION LIBRARY\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "## IMPORT UNET STRUCTURE FOR SEMI-SUPERVISED SEGMENTATION\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "## IMPORTING KRAFT DETECTOR RELATED MODULES FOR MONITOR-TEXT DETECTION\n",
    "from craft_text_detector import (\n",
    "    read_image,\n",
    "    load_craftnet_model,\n",
    "    load_refinenet_model,\n",
    "    get_prediction\n",
    ")\n",
    "\n",
    "## IMPORT OCR RELATED MODULES\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = \"cpu\"  ##using cpu for inference \n",
    "\n",
    "##input image dimension to be used for segmenation\n",
    "SEG_IN_IMAGE_HEIGHT = 320\n",
    "SEG_IN_IMAGE_WIDTH = 640\n",
    "\n",
    "##input image dimension to be used for classification\n",
    "CLASS_IN_IMAGE_HEIGHT = 360\n",
    "CLASS_IN_IMAGE_WIDTH = 640\n",
    "\n",
    "\n",
    "## LOADING TRANSFORMATION TO BE APPLIED TO IMAGE (AT INFERENCE TIME)\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p = 1.0), \n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING INDIVIDUAL PIPLEINE COMPONENTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. MONITOR SEGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading unet model\n",
    "seg_model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        \n",
    "    encoder_weights=\"imagenet\",     \n",
    "    in_channels=3,                  \n",
    "    classes=1,\n",
    ")\n",
    "\n",
    "seg_model = seg_model.to(DEVICE) ## adding to device\n",
    "seg_model.load_state_dict(torch.load(\"weights/unet.ckpt\")) ## loading model checkpoints (from our semi-supervised training)\n",
    "\n",
    "\n",
    "## function for predicting mask and upsampling it to original image size\n",
    "def maskPred(img):\n",
    "    seg_model.eval()\n",
    "    (orig_H, orig_W, _) = img.shape\n",
    "    \n",
    "    #resizing it to desired segmentation input\n",
    "    img = cv2.resize(img, (SEG_IN_IMAGE_WIDTH, SEG_IN_IMAGE_HEIGHT))\n",
    "    ## TRANSFORMING IMAGE\n",
    "    img = test_transform(image=img)[\"image\"]\n",
    "\n",
    "    ## RUNNING THROUGH MODEL\n",
    "    mask = seg_model(img.to(DEVICE).unsqueeze(0))\n",
    "    mask = mask.cpu().squeeze()\n",
    "    \n",
    "    ## RESIZING MASK (USAMPLING IT TO ORIGINAL SIZE)\n",
    "    mask = cv2.resize(np.uint8(mask>0)*255, (orig_W, orig_H))\n",
    "    return mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. PERSPECTIVE TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cell containing functions for improving perception and orientation of extracted monitor segments\n",
    "\n",
    "def order_points(pts):\n",
    "\t## function for sorting the corner points\n",
    "    xSorted = pts[np.argsort(pts[:, 0]), :]\n",
    "        \n",
    "    leftMost = xSorted[:2, :]\n",
    "    rightMost = xSorted[2:, :]\n",
    "        \n",
    "    leftMost = leftMost[np.argsort(leftMost[:, 1]), :]\n",
    "    (tl, bl) = leftMost\n",
    "        \n",
    "    D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0]\n",
    "    (br, tr) = rightMost[np.argsort(D)[::-1], :]\n",
    "        \n",
    "    return np.array([tl, tr, br, bl], dtype=\"float32\")\n",
    "\n",
    "\n",
    "## function for correcting the prespective (based on the principle of contour detection and convex hull)\n",
    "def correctPerspective(data, mask):\n",
    "\n",
    "    kernel = np.ones((20,20), np.uint8)  \n",
    "    mask = cv2.erode(mask, kernel, iterations=10)  \n",
    "    mask = cv2.dilate(mask, kernel, iterations=11)  \n",
    "\n",
    "    _, mask = cv2.threshold(mask, 70, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    mask = mask.astype(np.uint8)\n",
    "\n",
    "    cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    for count in cnts:\n",
    "        cnt = cv2.convexHull(count, False)\n",
    "        epsilon = 0.01 * cv2.arcLength(cnt, True)\n",
    "        approximations = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "        for ep in range(1, 5):\n",
    "            epsilon = ep*0.01 * cv2.arcLength(cnt, True)\n",
    "            approximations = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "            if len(approximations) == 4:\n",
    "                break\n",
    "        # img = cv2.drawContours(img, [approximations], 0, (0, 255, 0), 2)\n",
    "        \n",
    "        h, w = CLASS_IN_IMAGE_HEIGHT, CLASS_IN_IMAGE_WIDTH\n",
    "        orig_h, orig_w, _ = data.shape\n",
    "        \n",
    "        pt1 = np.float32([approximations[0][0], approximations[1][0], approximations[2][0], approximations[3][0]])\n",
    "        pt1 = order_points(pt1)\n",
    "        pt2 = np.float32([[0, 0], [w, 0], [w, h], [0, h]])\n",
    "        orig_pt = np.float32([[0, 0], [orig_w, 0], [orig_w, orig_h], [0, orig_h]])\n",
    "        matrix = cv2.getPerspectiveTransform(pt1, pt2)\n",
    "        orig_matrix = cv2.getPerspectiveTransform(pt1, orig_pt)\n",
    "        orig_op = cv2.warpPerspective(data, orig_matrix, (orig_w, orig_h))\n",
    "        shrink_op = cv2.warpPerspective(data, matrix, (w, h))\n",
    "\n",
    "        return (orig_op, shrink_op)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. MONITOR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cell containing resnet-18 for classifying 4 different monitor layouts\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.model = None\n",
    "        self.device = DEVICE\n",
    "    \n",
    "    def loadResnet18Classifier(self, chkpt_path):\n",
    "        self.model = models.resnet18()\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, self.num_classes)\n",
    "        checkpoint = torch.load(chkpt_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.model.to(self.device)\n",
    "        return\n",
    "\n",
    "    def predict(self, img):\n",
    "        self.model.eval()\n",
    "        img = test_transform(image=img)[\"image\"]\n",
    "        \n",
    "        ## RUNNING THROUGH MODEL\n",
    "        img = img.to(self.device).unsqueeze(0)\n",
    "        outputs = self.model(img)\n",
    "        _, preds = torch.max(F.softmax(outputs, dim = 1), 1)\n",
    "\n",
    "        return preds.cpu().numpy()[0]\n",
    "    \n",
    "def classification(img):\n",
    "    co = Classifier(4)\n",
    "    co.loadResnet18Classifier(\"weights/resnet18_weights\")\n",
    "    return co.predict(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. MONITOR TEXT DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cell for executing function predicting texts bounding boxes\n",
    "def get_boxes(image):\n",
    "  # image = read_image(img_path)\n",
    "  refine_net = load_refinenet_model(cuda= False)\n",
    "  craft_net = load_craftnet_model(cuda=False)\n",
    "  prediction_result = get_prediction(\n",
    "      image=image,\n",
    "      craft_net=craft_net,\n",
    "      refine_net=refine_net,\n",
    "      text_threshold=0.7,\n",
    "      link_threshold=0.4,\n",
    "      low_text=0.4,\n",
    "      cuda=True,\n",
    "      long_size=1280\n",
    "  )\n",
    "  return prediction_result['boxes']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\n",
    "ocr_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\n",
    "\n",
    "ocr_model = model.to(DEVICE)\n",
    "\n",
    "def trOCR(img, locs):\n",
    "    x, y, w, h = locs[0], locs[1], locs[2], locs[3]\n",
    "\n",
    "    x *= img.shape[1]\n",
    "    w *= img.shape[1]\n",
    "    y *= img.shape[0]\n",
    "    h *= img.shape[0]\n",
    "\n",
    "    crop = img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)]\n",
    "\n",
    "    pixel_values = processor(crop, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = torch.tensor(pixel_values).to(DEVICE)\n",
    "    \n",
    "    generated_ids = ocr_model.generate(pixel_values)\n",
    "    generated_text2 = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. RULE BASED CLASSIFCATION OF VITALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL INFERENCE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMG_DIR = \"test_data\"\n",
    "imlis = os.listdir(TEST_IMG_DIR)\n",
    "imloc = f\"{TEST_IMG_DIR}/{imlis[9]}\"\n",
    "\n",
    "def inference(image_path):\n",
    "\n",
    "  '''\n",
    "  Function responsible for inference.\n",
    "  Args: \n",
    "    image_path: str, path to image file. eg. \"input/aveksha_micu_mon--209_2023_1_17_12_0_34.jpeg\"\n",
    "  Returns:\n",
    "    result: dict, final output dictionary. eg. {\"HR\":\"80\", \"SPO2\":\"98\", \"RR\":\"15\", \"SBP\":\"126\", \"DBP\":\"86\"}\n",
    "  '''\n",
    "  result = {}\n",
    "\n",
    "  ### put your code here\n",
    "  \n",
    "  ## EVALUATION ON EACH IMAGE\n",
    "  image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "  mask = maskPred(image)\n",
    "  # print(mask)\n",
    "  (detector_io, class_io) =  correctPerspective(image, mask)\n",
    "  label = classification(class_io) \n",
    "\n",
    "  bounding_boxes = get_boxes(detector_io)\n",
    "\n",
    "  # print(bounding_boxes)\n",
    "\n",
    "  # fig, ax = plt.subplots(len(bounding_boxes) + 1)\n",
    "\n",
    "  # for idx,bounding_box in enumerate(bounding_boxes):\n",
    "  #     text, crop = OCR.trOCR(detector_io, bounding_box)\n",
    "\n",
    "  #     print(text)\n",
    "  #     ax[idx].imshow(crop)\n",
    "  # ax[-1].imshow(monitor)\n",
    "    \n",
    "  return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16 (default, Dec  7 2022, 01:12:13) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddd4d059d57b24667b9aec599ce6ca65a74fe89275a814d656e3c17a4a9e6734"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
