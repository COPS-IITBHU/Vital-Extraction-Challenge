{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "!pip install torchmetrics\n",
    "!pip install paddleocr>=2.0.1 paddlepaddle\n",
    "!pip install opencv-python-headless==4.5.3.56\n",
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STANDARD LIBRARY AND PIPELINE MODULES IMPORT\n",
    "\n",
    "\n",
    "##IMPORTING IMAGE LOADING AND PLOTTING LIBS\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "## IMPORTING TORCH FOR ML-PART\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## IMPORT ALBUMENTATION AS DATA AUGMENTATION LIBRARY\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "## IMPORT UNET STRUCTURE FOR SEMI-SUPERVISED SEGMENTATION\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "## IMPORTING PADDLEOCR RELATED MODULES FOR MONITOR-TEXT DETECTION AND OCR\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "## stop logging\n",
    "import logging\n",
    "logger = logging.getLogger('paddle')\n",
    "logger.disabled = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = \"cpu\"  ##using cpu for inference \n",
    "\n",
    "##input image dimension to be used for segmenation\n",
    "SEG_IN_IMAGE_HEIGHT = 320\n",
    "SEG_IN_IMAGE_WIDTH = 640\n",
    "\n",
    "##input image dimension to be used for classification\n",
    "CLASS_IN_IMAGE_HEIGHT = 360\n",
    "CLASS_IN_IMAGE_WIDTH = 640\n",
    "\n",
    "##input image dimension to be used for most ocr\n",
    "OCR_IN_IMAGE_HEIGHT = 180\n",
    "OCR_IN_IMAGE_WIDTH = 320\n",
    "\n",
    "\n",
    "## LOADING TRANSFORMATION TO BE APPLIED TO IMAGE (AT INFERENCE TIME)\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p = 1.0), \n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING INDIVIDUAL PIPLEINE COMPONENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. MONITOR SEGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading unet model\n",
    "seg_model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        \n",
    "    encoder_weights=\"imagenet\",     \n",
    "    in_channels=3,                  \n",
    "    classes=1,\n",
    ")\n",
    "\n",
    "seg_model = seg_model.to(DEVICE) ## adding to device\n",
    "seg_model.load_state_dict(torch.load(\"weights/unet.ckpt\", map_location=DEVICE)) ## loading model checkpoints (from our semi-supervised training)\n",
    "\n",
    "\n",
    "## function for predicting mask and upsampling it to original image size\n",
    "def maskPred(img):\n",
    "    seg_model.eval()\n",
    "    (orig_H, orig_W, _) = img.shape\n",
    "    \n",
    "    #resizing it to desired segmentation input\n",
    "    img = cv2.resize(img, (SEG_IN_IMAGE_WIDTH, SEG_IN_IMAGE_HEIGHT))\n",
    "    ## TRANSFORMING IMAGE\n",
    "    img = test_transform(image=img)[\"image\"]\n",
    "\n",
    "    ## RUNNING THROUGH MODEL\n",
    "    mask = seg_model(img.to(DEVICE).unsqueeze(0))\n",
    "    mask = mask.cpu().squeeze()\n",
    "    \n",
    "    ## RESIZING MASK (USAMPLING IT TO ORIGINAL SIZE)\n",
    "    mask = cv2.resize(np.uint8(mask>0)*255, (orig_W, orig_H))\n",
    "    return mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. PERSPECTIVE TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "\t\n",
    "    xSorted = pts[np.argsort(pts[:, 0]), :]\n",
    "        \n",
    "    leftMost = xSorted[:2, :]\n",
    "    rightMost = xSorted[2:, :]\n",
    "        \n",
    "    leftMost = leftMost[np.argsort(leftMost[:, 1]), :]\n",
    "    (tl, bl) = leftMost\n",
    "        \n",
    "    D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0]\n",
    "    (br, tr) = rightMost[np.argsort(D)[::-1], :]\n",
    "        \n",
    "    return np.array([tl, tr, br, bl], dtype=\"float32\")\n",
    "\n",
    "\n",
    "def correctPerspective(data, mask):\n",
    "\n",
    "    kernel = np.ones((20,20), np.uint8)  \n",
    "    mask = cv2.erode(mask, kernel, iterations=10)  \n",
    "    mask = cv2.dilate(mask, kernel, iterations=11)  \n",
    "\n",
    "    _, mask = cv2.threshold(mask, 70, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    mask = mask.astype(np.uint8)\n",
    "\n",
    "    cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    for count in cnts:\n",
    "        cnt = cv2.convexHull(count, False)\n",
    "        epsilon = 0.01 * cv2.arcLength(cnt, True)\n",
    "        approximations = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "        for ep in range(1, 5):\n",
    "            epsilon = ep*0.01 * cv2.arcLength(cnt, True)\n",
    "            approximations = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "            if len(approximations) == 4:\n",
    "                break\n",
    "        # img = cv2.drawContours(img, [approximations], 0, (0, 255, 0), 2)\n",
    "        \n",
    "        h, w = CLASS_IN_IMAGE_HEIGHT, CLASS_IN_IMAGE_WIDTH\n",
    "        orig_h, orig_w = 720,1280\n",
    "        h_, w_ = OCR_IN_IMAGE_HEIGHT, OCR_IN_IMAGE_WIDTH\n",
    "        pt1 = np.float32([approximations[0][0], approximations[1][0], approximations[2][0], approximations[3][0]])\n",
    "        pt1 = order_points(pt1)\n",
    "        pt2 = np.float32([[0, 0], [w, 0], [w, h], [0, h]])\n",
    "        orig_pt = np.float32([[0, 0], [orig_w, 0], [orig_w, orig_h], [0, orig_h]])\n",
    "        es_pt = np.float32([[0, 0], [w_, 0], [w_, h_], [0, h_]])\n",
    "        matrix = cv2.getPerspectiveTransform(pt1, pt2)\n",
    "        orig_matrix = cv2.getPerspectiveTransform(pt1, orig_pt)\n",
    "        es_matrix = cv2.getPerspectiveTransform(pt1, es_pt)\n",
    "        orig_op = cv2.warpPerspective(data, orig_matrix, (orig_w, orig_h))\n",
    "        shrink_op = cv2.warpPerspective(data, matrix, (w, h))\n",
    "        es_op = cv2.warpPerspective(data, es_matrix, (w_, h_))\n",
    "\n",
    "        return (es_op, shrink_op, orig_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. MONITOR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.model = None\n",
    "    \n",
    "    def loadResnet18Classifier(self, chkpt_path):\n",
    "        self.model = models.resnet18()\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, self.num_classes)\n",
    "        checkpoint = torch.load(chkpt_path, map_location=DEVICE)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "        return\n",
    "\n",
    "    def predict(self, img):\n",
    "        img = test_transform(image=img)[\"image\"]\n",
    "        \n",
    "        ## RUNNING THROUGH MODEL\n",
    "        img = img.to(DEVICE).unsqueeze(0)\n",
    "        outputs = self.model(img)\n",
    "        _, preds = torch.max(F.softmax(outputs, dim = 1), 1)\n",
    "\n",
    "        return preds.cpu().numpy()[0]\n",
    "\n",
    "co = Classifier(4)\n",
    "co.loadResnet18Classifier(\"weights/resnet18_weights\")    \n",
    "\n",
    "def classification(img):\n",
    "    return co.predict(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. MONITOR TEXT DETECTION AND OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023/02/06 00:48:59] ppocr DEBUG: Namespace(alpha=1.0, benchmark=False, beta=1.0, cls_batch_num=6, cls_image_shape='3, 48, 192', cls_model_dir='/home/drobot/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_thresh=0.9, cpu_threads=10, crop_res_save_dir='./output', det=True, det_algorithm='DB', det_box_type='quad', det_db_box_thresh=0.6, det_db_score_mode='fast', det_db_thresh=0.3, det_db_unclip_ratio=1.5, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_limit_side_len=960, det_limit_type='max', det_model_dir='/home/drobot/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, det_pse_thresh=0, det_sast_nms_thresh=0.2, det_sast_score_thresh=0.5, draw_img_save_dir='./inference_results', drop_score=0.4, e2e_algorithm='PGNet', e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_limit_side_len=768, e2e_limit_type='max', e2e_model_dir=None, e2e_pgnet_mode='fast', e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set='totaltext', enable_mkldnn=False, fourier_degree=5, gpu_mem=500, help='==SUPPRESS==', image_dir=None, image_orientation=False, ir_optim=True, kie_algorithm='LayoutXLM', label_list=['0', '180'], lang='en', layout=True, layout_dict_path=None, layout_model_dir=None, layout_nms_threshold=0.5, layout_score_threshold=0.5, max_batch_size=10, max_text_length=25, merge_no_span_structure=True, min_subgraph_size=15, mode='structure', ocr=True, ocr_order_method=None, ocr_version='PP-OCRv3', output='./output', page_num=0, precision='fp32', process_id=0, re_model_dir=None, rec=True, rec_algorithm='SVTR_LCNet', rec_batch_num=6, rec_char_dict_path='/home/drobot/Documents/iot_ec423_paper/venv/lib/python3.8/site-packages/paddleocr/ppocr/utils/en_dict.txt', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_model_dir='/home/drobot/.paddleocr/whl/rec/en/en_PP-OCRv3_rec_infer', recovery=False, save_crop_res=False, save_log_path='./log_output/', scales=[8, 16, 32], ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ser_model_dir=None, show_log=True, sr_batch_num=1, sr_image_shape='3, 32, 128', sr_model_dir=None, structure_version='PP-StructureV2', table=True, table_algorithm='TableAttn', table_char_dict_path=None, table_max_len=488, table_model_dir=None, total_process_num=1, type='ocr', use_angle_cls=False, use_dilation=False, use_gpu=False, use_mp=False, use_npu=False, use_onnx=False, use_pdf2docx_api=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, use_visual_backbone=True, use_xpu=False, vis_font_path='./doc/fonts/simfang.ttf', warmup=False)\n"
     ]
    }
   ],
   "source": [
    "## cell for executing function predicting texts bounding boxes using paddleocr\n",
    "ocr_m = PaddleOCR(lang='en', use_gpu=(DEVICE==\"gpu\"), det_db_box_thresh=0.6, drop_score = 0.4) # need to run only once to download and load model into memory\n",
    "\n",
    "def get_text(crop_img, det=False):\n",
    "    return ocr_m.ocr(crop_img, det=det)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. RULE BASED CLASSIFCATION OF VITALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rules:   \n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _txt_len(self, text):\n",
    "        ##avoiding string containing long texts as it will be generally something else\n",
    "        if len(text)>7:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _is_alpha(self, text):\n",
    "        ##avoiding string containing alphabets\n",
    "        if re.search('\\d+', text)==None:\n",
    "            return False\n",
    "        return True\n",
    "    def _pre_check(self, text):\n",
    "        ## pre checking for valid characters\n",
    "        valid = True\n",
    "        valid = (self._txt_len(text) and valid)\n",
    "        valid = (self._is_alpha(text) and valid)\n",
    "        return valid\n",
    "\n",
    "    def BPRule(self, text):\n",
    "        sbp = 0\n",
    "        dbp = 0\n",
    "        pos = text.find(\"/\")\n",
    "        # print(\"POS - \", pos)\n",
    "        if pos!=-1:\n",
    "            sbp = text[max(pos-3, 0):pos]\n",
    "            dbp = text[pos+1:pos+4]\n",
    "            return True, [sbp, dbp]\n",
    "        else:\n",
    "            return False, None\n",
    "\n",
    "    def check_green(self, cropped):\n",
    "        normalizedImg = np.zeros((cropped.shape[0], cropped.shape[1]))\n",
    "        cropped = cv2.normalize(cropped,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
    "        hsv = cv2.cvtColor(cropped, cv2.COLOR_RGB2HSV)\n",
    "        mask = cv2.inRange(hsv, (40, 40, 50), (80, 255,255))\n",
    "        imask = mask>0\n",
    "        area = cropped.shape[0]*cropped.shape[1]\n",
    "        thres = 0.16\n",
    "#         print(imask.sum()/area)\n",
    "        if (imask.sum()/area) > thres and area > self.min_area:    \n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def check_map(self, text):\n",
    "        if re.search('\\(.*\\d+\\)', text)==None:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def check_yellow(self, cropped):\n",
    "        normalizedImg = np.zeros((cropped.shape[0], cropped.shape[1]))\n",
    "        cropped = cv2.normalize(cropped,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
    "        hsv = cv2.cvtColor(cropped, cv2.COLOR_RGB2HSV)\n",
    "        mask = cv2.inRange(hsv, (20, 50, 50), (40, 255, 255))\n",
    "        imask = mask>0\n",
    "        area = cropped.shape[0]*cropped.shape[1]\n",
    "#         print(imask.sum()/area)\n",
    "        thres = 0.15\n",
    "        if (imask.sum()/area) > thres and area > self.min_area:    \n",
    "            return True\n",
    "        return False\n",
    "    def check_cyan(self, cropped):\n",
    "        normalizedImg = np.zeros((cropped.shape[0], cropped.shape[1]))\n",
    "        cropped = cv2.normalize(cropped,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
    "        hsv = cv2.cvtColor(cropped, cv2.COLOR_RGB2HSV)\n",
    "        mask = cv2.inRange(hsv, (80, 50, 60), (100, 255,255))\n",
    "        imask = mask>0\n",
    "        area = cropped.shape[0]*cropped.shape[1]\n",
    "        thres = 0.15        \n",
    "        if (imask.sum()/area) > thres and area > self.min_area:    \n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def class_pred(self, text, cropped, label):\n",
    "        self.min_area = 300\n",
    "        if label == 1:\n",
    "            self.min_area = 700\n",
    "        valid = self._pre_check(text)\n",
    "        \n",
    "        if valid:\n",
    "            hr = self.check_green(cropped)\n",
    "            map_check = self.check_map(text)\n",
    "            yellow = self.check_yellow(cropped)\n",
    "            cyan = self.check_cyan(cropped)\n",
    "            bp = self.BPRule(text)\n",
    "            if bp[0]:\n",
    "                return \"BP\", [(\"SBP\", bp[1][0]), (\"DBP\", bp[1][1])]\n",
    "            elif hr:\n",
    "                return \"HR\", [text]\n",
    "            # elif map_check:\n",
    "            #     return \"MAP\", int(text[1:len(text)-1])\n",
    "            elif yellow:\n",
    "                return \"yellow\", (text)\n",
    "            elif cyan:\n",
    "                return 'cyan', (text)\n",
    "            else:\n",
    "                return \"\"     \n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "def find_nearby(slash_box, boxes):\n",
    "    X, Y, W, H, _ = slash_box\n",
    "    ans_index = -1\n",
    "    min_del_y = 1e5\n",
    "    for i in range(len(boxes)):\n",
    "        x,y,w,h, _ = boxes[i]\n",
    "        if x < X:\n",
    "            if min_del_y > abs(y-Y):\n",
    "                min_del_y = abs(y-Y)\n",
    "                ans_index = i\n",
    "    return ans_index\n",
    "\n",
    "ru = Rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL INFERENCE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image_path):\n",
    "\n",
    "    '''\n",
    "    Function responsible for inference.\n",
    "    Args: \n",
    "      image_path: str, path to image file. eg. \"input/aveksha_micu_mon--209_2023_1_17_12_0_34.jpeg\"\n",
    "    Returns:\n",
    "      result: dict, final output dictionary. eg. {\"HR\":\"80\", \"SPO2\":\"98\", \"RR\":\"15\", \"SBP\":\"126\", \"DBP\":\"86\"}\n",
    "    '''\n",
    "    result = {}\n",
    "\n",
    "    ### put your code here\n",
    "    \n",
    "    ## EVALUATION ON EACH IMAGE\n",
    "\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    ocr_io = cv2.resize(image, (320, 180))\n",
    "    class_io = cv2.resize(image, (640, 360))\n",
    "\n",
    "\n",
    "    # mask = maskPred(image)\n",
    "    # # print(mask)\n",
    "    # (ocr_io, class_io, orig_io) =  correctPerspective(image, mask)\n",
    "    label = classification(class_io) \n",
    "\n",
    "    #label = 2\n",
    "    if label==1:\n",
    "        candidates =  get_text(class_io, det=True)\n",
    "        ocr_io = class_io\n",
    "    elif label == 3:\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        ocr_io = cv2.filter2D(ocr_io, -1, kernel)\n",
    "        \n",
    "        candidates =  get_text(ocr_io[:, :115], det=True)\n",
    "    else:\n",
    "        # print(\"here\")\n",
    "        candidates =  get_text(ocr_io, det=True)\n",
    "    can = candidates[0]\n",
    "    # plt.imshow(ocr_io[:, :115])\n",
    "    # plt.show()\n",
    "    hr_boxes = []\n",
    "\n",
    "    boxes = []\n",
    "    yellow_boxes = []\n",
    "    slash_box = []\n",
    "    for line in can:\n",
    "        bbox = line[0]\n",
    "        txt = line[1][0]\n",
    "        confidence = line[1][1]\n",
    "        \n",
    "        x1, y1, x2, y2 = int(bbox[0][0]),int(bbox[0][1]) , int(bbox[2][0]),int(bbox[2][1])\n",
    "        cropped = ocr_io[y1: y2, x1:x2]\n",
    "        # print(x1, y1,x2,y2)\n",
    "        # print(txt)\n",
    "        # plt.imshow(cropped)\n",
    "        # plt.show()\n",
    "        pred = ru.class_pred(txt, cropped, label)\n",
    "        if len(re.findall(\"^/\\d+\", txt.strip()))>0:\n",
    "            slash_box = [(x1+x2)/2, (y1+y2)/2, x2-x1,y2-y1, txt]\n",
    "        else:\n",
    "            boxes.append([(x1+x2)/2, (y1+y2)/2, x2-x1,y2-y1, txt ])\n",
    "        if len(pred)>0:\n",
    "            clas = pred[0]\n",
    "            if(clas==\"BP\"):\n",
    "                result[pred[1][0][0]] = pred[1][0][1]\n",
    "                result[pred[1][1][0]] = pred[1][1][1]\n",
    "            elif(clas == 'HR'):\n",
    "                hr_boxes.append([cropped.shape[0]*cropped.shape[1], [x1,y1,x2,y2], pred])\n",
    "            elif(clas == 'cyan'):\n",
    "                if label == 1:\n",
    "                    result['RR'] = pred[1]\n",
    "                elif label==0 or label == 2: \n",
    "                    result['SPO2'] = pred[1]\n",
    "            elif(clas=='yellow'):\n",
    "                if label == 1:\n",
    "                    yellow_boxes.append([(y1+y2)/2, cropped.shape[0]*cropped.shape[1], [x1,y1,x2,y2], txt])\n",
    "                elif label == 0 or label == 2:\n",
    "                    result['RR'] = txt\n",
    "                elif label == 3:\n",
    "                    result['SPO2'] = txt\n",
    "                    \n",
    "    #         print(txt ,\"-\", len(pred))\n",
    "    yellow_boxes = sorted(yellow_boxes, key=lambda x: x[0], reverse=True)\n",
    "    hr_boxes = sorted(hr_boxes, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if label == 1 and len(yellow_boxes) > 0:\n",
    "        result['SPO2'] = yellow_boxes[0][3]\n",
    "\n",
    "    if len(hr_boxes)>0:\n",
    "        x1,y1,x2,y2 = hr_boxes[0][1]\n",
    "        hr = (hr_boxes[0][2][1][0])\n",
    "        a = re.findall('^\\d+', hr)\n",
    "        hr = int(a[0])\n",
    "        if int(a[0][:3]) > 220:\n",
    "            hr = int(a[0][:2])\n",
    "        elif int(a[0]) > 300:\n",
    "            hr = int(a[0][:3])\n",
    "        result['HR'] = hr\n",
    "    #         plt.imshow(ocr_io[y1: y2, x1:x2])\n",
    "    #         plt.show()\n",
    "\n",
    "    if len(slash_box) > 0:\n",
    "        index = find_nearby(slash_box, boxes)\n",
    "        x, y, w, h, pred = boxes[index]\n",
    "        \n",
    "        result['SBP'] = pred.strip()\n",
    "        result['DBP'] = slash_box[-1][1:]\n",
    "\n",
    "    boxes = sorted(boxes, key= lambda x: x[1], reverse=True)\n",
    "    if label == 3:\n",
    "        for (x,y,w,h,pred) in boxes:\n",
    "            if w*h > 200:\n",
    "                if pred[1] == 'T':\n",
    "                    pred = pred[0] + '7'\n",
    "                result['RR'] = pred\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    # print(result)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023/02/06 00:58:55] ppocr WARNING: Since the angle classifier is not initialized, the angle classifier will not be uesd during the forward process\n",
      "[2023/02/06 00:58:55] ppocr DEBUG: dt_boxes num : 12, elapse : 0.04934191703796387\n",
      "[2023/02/06 00:58:56] ppocr DEBUG: rec_res num  : 12, elapse : 0.5578830242156982\n",
      "CPU times: user 1.26 s, sys: 44.7 ms, total: 1.31 s\n",
      "Wall time: 699 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SBP': '115', 'DBP': '70', 'SPO2': '95', 'RR': '17', 'HR': 87}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "img_classes = [ 'BPL-EliteView-EV10-B_Meditec-England-A', 'BPL-EliteView-EV100-C' , 'BPL-Ultima-PrimeD-A' , 'Nihon-Kohden-lifescope-A']\n",
    "TEST_IMG_DIR = f\"val/{img_classes[0]}\"\n",
    "imlis = os.listdir(TEST_IMG_DIR)\n",
    "imloc = f\"{TEST_IMG_DIR}/{imlis[5]}\"\n",
    "\n",
    "inference(imloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8aeaf086f52e7bc0582dbecc835cc70d8a3cac9ba600dbd1f9c384478a46b68a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
